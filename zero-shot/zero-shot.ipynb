{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Zero-shot-learning\" data-toc-modified-id=\"Zero-shot-learning-0.1\">Zero-shot learning</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1\">Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-word2vec\" data-toc-modified-id=\"Download-word2vec-1.1\">Download word2vec</a></span></li><li><span><a href=\"#Download-cifar10\" data-toc-modified-id=\"Download-cifar10-1.2\">Download cifar10</a></span></li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-1.3\">Train-test split</a></span></li><li><span><a href=\"#Label-embeddings\" data-toc-modified-id=\"Label-embeddings-1.4\">Label embeddings</a></span></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\">Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pretrained-VGG19\" data-toc-modified-id=\"Pretrained-VGG19-2.1\">Pretrained VGG19</a></span></li><li><span><a href=\"#Embeddings-prediction-model\" data-toc-modified-id=\"Embeddings-prediction-model-2.2\">Embeddings prediction model</a></span></li></ul></li><li><span><a href=\"#Train-model\" data-toc-modified-id=\"Train-model-3\">Train model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Most-similar\" data-toc-modified-id=\"Most-similar-3.1\">Most similar</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot learning\n",
    "\n",
    "Some links:\n",
    "* [DeViSE: A Deep Visual-Semantic Embedding Model](https://papers.nips.cc/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf)\n",
    "* [DeViSE Zero-shot learning](https://towardsdatascience.com/devise-zero-shot-learning-c62eed17e93d)\n",
    "* [Zero-shot learning research by Max Planck Institute](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/zero-shot-learning/)\n",
    "* [Zero-shot Learning: An Introduction](https://www.learnopencv.com/zero-shot-learning-an-introduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.0', '2.2.4-tf')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Download word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run load only once and store data on a disk\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "# wv.save(\"word2vec-google-news-300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "# load word2vec model\n",
    "wv = Word2VecKeyedVectors.load(\"word2vec-google-news-300.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar 10 dataset which contains images with 10 classes\n",
    "\n",
    "data = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airplane', 0),\n",
       " ('automobile', 1),\n",
       " ('bird', 2),\n",
       " ('cat', 3),\n",
       " ('deer', 4),\n",
       " ('dog', 5),\n",
       " ('frog', 6),\n",
       " ('horse', 7),\n",
       " ('ship', 8),\n",
       " ('truck', 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "    'ship', 'truck'\n",
    "]\n",
    "\n",
    "list(zip(class_names, np.unique(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "Split data by labels with test set containing only unseen labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: (35000, 32, 32, 3), labels: (35000,)\n",
      "Test features: (3000, 32, 32, 3), labels: (3000,)\n"
     ]
    }
   ],
   "source": [
    "split_label = 'frog'\n",
    "split_idx = class_names.index(split_label)\n",
    "\n",
    "X_train = train_images[(train_labels <= split_idx).ravel()]\n",
    "y_train = train_labels[train_labels <= split_idx]\n",
    "print(f'Train features: {X_train.shape}, labels: {y_train.shape}')\n",
    "\n",
    "X_test = test_images[(test_labels > split_idx).ravel()]\n",
    "y_test = test_labels[test_labels > split_idx]\n",
    "print(f'Test features: {X_test.shape}, labels: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use word2vec model to translate labels (strings) to vectors of numbers\n",
    "y_train = np.array([wv[class_names[y]] for y in y_train])\n",
    "y_test = np.array([wv[class_names[y]] for y in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Pretrained VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set include_top to False to use different input image size\n",
    "\n",
    "vgg19 = tf.keras.applications.VGG19(\n",
    "    weights='imagenet', include_top=False, input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_text_embeddings(img_model, num_classes):\n",
    "    \"\"\"Extend VGG19 by adding few fully connected layers and transform\n",
    "    the model to regressor\n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.models.Sequential(img_model.layers[:-1])\n",
    "    \n",
    "    model.add(keras.layers.Flatten(input_shape=[32, 32]))\n",
    "    \n",
    "    # use BatchNormalization as regularization and to avoid additional normalization\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               77100     \n",
      "=================================================================\n",
      "Total params: 20,635,244\n",
      "Trainable params: 20,630,636\n",
      "Non-trainable params: 4,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = img_to_text_embeddings(vgg19, 300)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # important part - we want to minimize simularity\n",
    "    # between word vectors\n",
    "    loss=keras.losses.cosine_similarity,\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['mean_absolute_percentage_error',\n",
    "             'mean_absolute_error']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 40s 40ms/sample - loss: -0.3919 - mean_absolute_percentage_error: 111.5794 - mean_absolute_error: 0.1427\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 40s 40ms/sample - loss: -0.3816 - mean_absolute_percentage_error: 112.6684 - mean_absolute_error: 0.1427\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 40s 40ms/sample - loss: -0.4008 - mean_absolute_percentage_error: 111.8886 - mean_absolute_error: 0.1426\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 40s 40ms/sample - loss: -0.3942 - mean_absolute_percentage_error: 111.9585 - mean_absolute_error: 0.1426\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 42s 42ms/sample - loss: -0.4021 - mean_absolute_percentage_error: 112.3430 - mean_absolute_error: 0.1426\n"
     ]
    }
   ],
   "source": [
    "# use a small subset of the data\n",
    "history = model.fit(X_train[:1000], y_train[:1000], epochs=5)\n",
    "# history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_train[:100].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.39669716e-05, 7.04840249e-06, 2.39799672e-04, ...,\n",
       "        6.51157461e-06, 4.89254540e-04, 7.63902699e-06],\n",
       "       [2.97556422e-03, 5.59292792e-04, 2.31011261e-04, ...,\n",
       "        1.58765574e-03, 3.93970491e-04, 8.67550552e-04],\n",
       "       [9.26235954e-36, 2.70010446e-33, 2.36223104e-06, ...,\n",
       "        0.00000000e+00, 7.44081126e-06, 2.64058297e-36],\n",
       "       ...,\n",
       "       [1.57814687e-19, 1.24880346e-18, 5.17695953e-05, ...,\n",
       "        1.56849365e-22, 1.34593967e-04, 4.70326721e-20],\n",
       "       [1.52692127e-27, 7.26766212e-26, 1.27805988e-05, ...,\n",
       "        7.17907902e-32, 3.65746091e-05, 4.44827465e-28],\n",
       "       [1.10308835e-02, 2.56429776e-03, 5.96170066e-05, ...,\n",
       "        1.26234954e-02, 5.57754320e-05, 4.33210330e-03]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted embeddings\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most similar\n",
    "\n",
    "Find most similar embeddings. These embeddings are candidate labels for an image with unseen label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar = wv.most_similar(pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
